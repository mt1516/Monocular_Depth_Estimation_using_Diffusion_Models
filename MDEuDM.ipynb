{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17009,"status":"ok","timestamp":1690969263409,"user":{"displayName":"Melvin Tong (Melvintong)","userId":"09425469311594841840"},"user_tz":-480},"id":"HVder2wTp101","outputId":"977260e6-7885-42b6-fb02-17c34ac637e0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/mt1516/anaconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/mt1516/anaconda3/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n","  Referenced from: <6A7076EE-85BD-37A7-BC35-1D4867F2B3D3> /Users/mt1516/anaconda3/lib/python3.10/site-packages/torchvision/image.so\n","  Expected in:     <F2FE5CF8-5B5B-3FAD-ADF8-C77D90F49FC9> /Users/mt1516/anaconda3/lib/python3.10/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n","  warn(\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import h5py\n","import pandas as pd\n","import math\n","from tqdm import tqdm\n","from scipy.interpolate import NearestNDInterpolator\n","import gc\n","import torch\n","from torch import nn, einsum\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage\n","from torchvision.utils import save_image\n","# !pip install einops\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","import psutil"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5629,"status":"ok","timestamp":1690969268728,"user":{"displayName":"Melvin Tong (Melvintong)","userId":"09425469311594841840"},"user_tz":-480},"id":"ua6oYOPS0-l2","outputId":"da7ae212-c807-4993-f852-dd7db029c79d"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# # %cd \"/content/drive/My Drive/Colab Notebooks/MDEuDM/\"\n","# %cd \"/content/\""]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1690961970676,"user":{"displayName":"Melvin Tong (Melvintong)","userId":"09425469311594841840"},"user_tz":-480},"id":"1iglOm8Yp101"},"outputs":[],"source":["# !wget http://horatio.cs.nyu.edu/mit/silberman/nyu_depth_v2/nyu_depth_v2_labeled.mat -P ./dataset/NYUDv2/\n","# # sku_mat = h5py.File('./drive/My Drive/Colab Notebooks/MDEuDM/dataset/NYUDv2/nyu_depth_v2_labeled.mat', 'r')\n","# sku_mat = h5py.File('./dataset/NYUDv2/nyu_depth_v2_labeled.mat', 'r')"]},{"cell_type":"markdown","metadata":{"id":"NjYTlfP-p102"},"source":["### Data Prepressing"]},{"cell_type":"markdown","metadata":{"id":"OABZ725Ep103"},"source":["Inpaint training data of missing values with nearest neighbors."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1690961970677,"user":{"displayName":"Melvin Tong (Melvintong)","userId":"09425469311594841840"},"user_tz":-480},"id":"7veyr4EBp104"},"outputs":[],"source":["# def inpaint_missing_processor(data, rgb=False):\n","#     \"\"\"Inpaint missing depth values using nearest neighbor interpolation\"\"\"\n","#     # Create a mask of values\n","#     if rgb:\n","#         mask = data == 255\n","#     else:\n","#         mask = data == 0\n","#     # Create a nearest neighbor interpolator\n","#     interp = NearestNDInterpolator(np.argwhere(~mask), np.array(data)[~mask])\n","#     # Interpolate the values\n","#     infilled = interp(np.argwhere(mask))\n","#     # Replace the zeros with the interpolated values\n","#     if rgb:\n","#         data[mask] = infilled.astype(np.uint8)\n","#     else:\n","#         data[mask] = infilled.astype(np.float32)\n","#     return data\n","\n","# def inpaint_missing(data, rgb=False):\n","#     if rgb:\n","#         for i in range(3):\n","#             data[:,:,i] = inpaint_missing_processor(data[:,:,i], rgb=True)\n","#         return data\n","#     return inpaint_missing_processor(data)\n","\n","\n","# def horizontal_flip(data, rgb=False):\n","#     if rgb:\n","#         for i in range(3):\n","#             data[:,:,i] = np.fliplr(data[:,:,i])\n","#         return data\n","#     return np.fliplr(data)\n","# # # testing = sku_mat['images'][6].T\n","# # # save the image\n","# # # import skimage.io as io\n","# # # io.imsave('./dataset/NYUDv2/image_cleaned/' + str(6) + '.png', testing)\n","# # # # io.imshow(testing)\n","# # # # plt.show()\n","# # # # testing_ = inpaint_missing(testing)\n","# # # # io.imshow(testing_)\n","# # # # plt.show()\n","# # # # io.imshow(sku_mat['depths'][6].T)\n","# # # # plt.show()\n","# # # # image = sku_mat['images'][6].T\n","# # # # io.imshow(image)\n","# # # # plt.show()\n","# # # # image_ = inpaint_missing(image, rgb=True)\n","# # # # io.imshow(image_)\n","# # # # plt.show()\n","\n","# for i, depth_photo in enumerate(sku_mat['rawDepths']):\n","#     depth_photo = inpaint_missing(depth_photo.T)\n","#     # io.imsave('./dataset/NYUDv2/depth_cleaned/' + str(i) + '.png', depth_photo)\n","#     depth_photo = cv2.resize(depth_photo, (320, 240), interpolation=cv2.INTER_NEAREST)\n","#     np.save('./dataset/NYUDv2/depth_cleaned_resized/' + str(i) + '.npy', depth_photo)\n","#     np.save('./dataset/NYUDv2/depth_cleaned_resized/' + str(i) + '_flipped.npy', horizontal_flip(depth_photo))\n","#     # np.save('./drive/MyDrive/Colab Notebooks/MDEuDM/dataset/NYUDv2/depth_cleaned_resized/' + str(i) + '.npy', depth_photo)\n","#     # np.save('./drive/MyDrive/Colab Notebooks/MDEuDM/dataset/NYUDv2/depth_cleaned_resized/' + str(i) + '_flipped.npy', horizontal_flip(depth_photo))\n","\n","# for i, rgb_photo in enumerate(sku_mat['images']):\n","#     rgb_photo = inpaint_missing(rgb_photo.T, rgb=True)\n","#     # io.imsave('./dataset/NYUDv2/image_cleaned' + str(i) + '.png', rgb_photo)\n","#     rgb_photo = cv2.resize(rgb_photo, (320, 240), interpolation=cv2.INTER_NEAREST)\n","#     np.save('./dataset/NYUDv2/image_cleaned_resized/' + str(i) + '.npy', rgb_photo)\n","#     np.save('./dataset/NYUDv2/image_cleaned_resized/' + str(i) + '_flipped.npy', horizontal_flip(rgb_photo, rgb=True))\n","#     # np.save('./drive/MyDrive/Colab Notebooks/MDEuDM/dataset/NYUDv2/image_cleaned_resized/' + str(i) + '.npy', rgb_photo)\n","#     # np.save('./drive/MyDrive/Colab Notebooks/MDEuDM/dataset/NYUDv2/image_cleaned_resized/' + str(i) + '_flipped.npy', horizontal_flip(rgb_photo, rgb=True))"]},{"cell_type":"markdown","metadata":{"id":"AeYBRJQ023H5"},"source":["### Zip"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1690961970679,"user":{"displayName":"Melvin Tong (Melvintong)","userId":"09425469311594841840"},"user_tz":-480},"id":"rdtyeMhG-JBz","outputId":"f6c1cdef-b808-4656-c6ca-a6c824a88822"},"outputs":[],"source":["# !zip -r ./depth_cleaned_resized.zip ./dataset/NYUDv2/depth_cleaned_resized\n","# !zip -r ./image_cleaned_resized.zip ./dataset/NYUDv2/image_cleaned_resized"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1690961971015,"user":{"displayName":"Melvin Tong (Melvintong)","userId":"09425469311594841840"},"user_tz":-480},"id":"0ss_qdF93xIp","outputId":"cd6abdd7-a3b4-435d-cc79-694a20a18a6d"},"outputs":[],"source":["# !unzip \"./depth_cleaned_resized.zip\" -d \"./drive/MyDrive/Colab Notebooks/MDEuDM/dataset/NYUDv2/depth_cleaned_resized/\"\n","# !unzip \"./image_cleaned_resized.zip\" -d \"./drive/MyDrive/Colab Notebooks/MDEuDM/dataset/NYUDv2/image_cleaned_resized/\""]},{"cell_type":"markdown","metadata":{"id":"xXY3Ccnl27ev"},"source":["### Model"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1690969268731,"user":{"displayName":"Melvin Tong (Melvintong)","userId":"09425469311594841840"},"user_tz":-480},"id":"xV0JcKRO26aO"},"outputs":[],"source":["class Upsample(nn.Module):\n","    '''\n","    Upsampling module\n","    '''\n","    # @torch.no_grad()\n","    def __init__(self, in_channels, out_channels, scale_factor=2, mode='nearest'):\n","        super().__init__()\n","        self.scale_factor = scale_factor\n","        self.mode = mode\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","\n","    # @torch.no_grad()\n","    def forward(self, x):\n","        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n","        x = self.conv(x)\n","        return x\n","\n","\n","class Downsample(nn.Module):\n","    '''\n","    Downsampling module\n","    '''\n","    # @torch.no_grad()\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n","\n","    # @torch.no_grad()\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class SinusoidalPositionEmbeddings(nn.Module):\n","    '''\n","    Time embedding module\n","    '''\n","    def __init__(self, dim=1024):\n","        super(SinusoidalPositionEmbeddings, self).__init__()\n","        self.dim = dim\n","\n","    def forward(self, time):\n","        device = time.device\n","        half_dim = self.dim // 2\n","        emb = math.log(10000) / (half_dim - 1)\n","        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n","        emb = time[:, None] * emb[None, :]\n","        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n","        return emb\n","\n","\n","class SelfAttention(nn.Module):\n","    '''\n","    Attention module\n","    '''\n","    def __init__(self, channel, heads=8):\n","        super(SelfAttention, self).__init__()\n","        self.heads = heads\n","        # hidden_dim = channel * 2\n","        self.scale = (channel * 2 // heads) ** -0.5\n","        self.norm = nn.GroupNorm(heads, channel)\n","        # self.norm = nn.GroupNorm(heads, hidden_dim)\n","        self.to_qkv = nn.Conv2d(channel, channel * 3, 1, bias=False)\n","        # self.to_qkv = nn.Conv2d(channel, hidden_dim * 3, 1, bias=False)\n","        self.to_out = nn.Conv2d(channel * 2, channel, 1)\n","\n","    def forward(self, x):\n","        b, c, h, w = x.shape\n","        x = self.norm(x)\n","        qkv = self.to_qkv(x).chunk(3, dim=1)\n","        q, k, v = map(\n","            lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h=self.heads), qkv\n","        )\n","        q = q * self.scale\n","        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n","        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n","        attn = sim.softmax(dim=-1)\n","        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n","        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x=h, y=w)\n","        return self.to_out(out)\n","\n","\n","# class LinearAttention(nn.Module):\n","#     '''\n","#     Linear attention block\n","#     '''\n","#     def __init__(self, dim, heads=8):\n","#         super(LinearAttention, self).__init__()\n","#         self.heads = heads\n","#         hidden_dim = dim * 2\n","#         self.scale = (hidden_dim // heads) ** -0.5\n","#         self.to_qkv = nn.Linear(dim, hidden_dim * 3, 1, bias=False)\n","#         self.to_out = nn.Linear(hidden_dim, dim)\n","\n","#     def forward(self, x):\n","#         b, c, h, w = x.shape\n","#         qkv = self.to_qkv(x).chunk(3, dim=1)\n","#         q, k, v = map(\n","#             lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h=self.heads), qkv\n","#         )\n","#         q = q.softmax(dim=-2)\n","#         k = k.softmax(dim=-1)\n","#         q = q * self.scale\n","#         context = einsum('b h d n, b h e n -> b h d e', k, v)\n","#         out = einsum('b h d e, b h d n -> b h e n', context, q)\n","#         out = rearrange(out, 'b h c (x y) -> b (h c) x y', h=self.heads, x=h, y=w)\n","#         return self.to_out(out)\n","\n","\n","# class WeightStandardizedConv2d(nn.Conv2d):\n","#     '''\n","#     Weight standardized convolutional layer\n","#     '''\n","#     def forward(self, x):\n","#         eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n","#         weight = self.weight\n","#         mean = reduce(weight, 'o ... -> o 1 1 1', 'mean')\n","#         var = reduce(weight, 'o ... -> o 1 1 1', partial(torch.var, unbiased=False))\n","#         normalized_weight = (weight - mean) / (var + eps).rsqrt()\n","#         return F.conv2d(x, normalized_weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n","\n","\n","class ResNetBlock(nn.Module):\n","    '''\n","    Residual convolutional block with GroupNorm and SiLU activation\n","    '''\n","    def __init__(self, channels, groups=8):\n","        super(ResNetBlock, self).__init__()\n","        self.conv_block = nn.Sequential(\n","            nn.GroupNorm(groups, channels),\n","            nn.SiLU(),\n","            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n","            nn.GroupNorm(groups, channels),\n","            nn.SiLU(),\n","            nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        )\n","        self.residual = nn.Conv2d(channels, channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv_block(x) + self.residual(x)\n","\n","\n","class EncoderBlock(nn.Module):\n","    '''\n","    Encoder: Downward convolutional block with GroupNorm and SiLU activation\n","    '''\n","    def __init__(self, in_channels, out_channels, strides=(2,2), num_resnet_block=8, allow_conv=False, allow_attention=False):\n","        super(EncoderBlock, self).__init__()\n","        self.conv = Upsample(in_channels, out_channels) if allow_conv else None\n","        self.mlp = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(256, out_channels * 2)\n","        )\n","        self.scaling = nn.Sequential(\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.GroupNorm(8, out_channels)\n","        )\n","        self.resnet_blocks = nn.ModuleList([ResNetBlock(out_channels) for _ in range(num_resnet_block)])\n","        self.attention = SelfAttention(out_channels) if allow_attention else None\n","        \n","\n","    def forward(self, x, time_emb=None):\n","        if self.conv is not None:\n","            # print('\\t', 'Conv_start', psutil.virtual_memory()[1]/1024000000)\n","            x = self.conv(x)\n","            # print('\\t', 'Conv_done', psutil.virtual_memory()[1]/1024000000)\n","        if self.mlp is not None:\n","        # print('\\t', 'mlp_start', psutil.virtual_memory()[1]/1024000000)\n","            time_emb = self.mlp(time_emb)\n","            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n","            scale_shift = time_emb.chunk(2, dim=1)\n","            x = self.scaling(x) * (scale_shift[0] + 1) + scale_shift[1]\n","            x = nn.SiLU()(x)\n","        # print('\\t', 'mlp_done', psutil.virtual_memory()[1]/1024000000)\n","        for block in self.resnet_blocks:\n","            # print('\\t', 'block_start', psutil.virtual_memory()[1]/1024000000)\n","            x = block(x)\n","            # print('\\t', 'block_done', psutil.virtual_memory()[1]/1024000000)\n","        if self.attention is not None:\n","            # print('\\t', 'attention_start', psutil.virtual_memory()[1]/1024000000)\n","            x = self.attention(x)\n","            # print('\\t', 'attention_done', psutil.virtual_memory()[1]/1024000000)\n","        return x\n","\n","\n","class DecoderBlock(nn.Module):\n","    '''\n","    Decoder: Upward convolutional block with GroupNorm and SiLU activation\n","    '''\n","    def __init__(self, in_channels, out_channels, strides=(2,2), num_resnet_block=8, allow_conv=False, allow_attention=False):\n","        super(DecoderBlock, self).__init__()\n","        self.mlp = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(256, in_channels * 2)\n","        )\n","        self.scaling = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n","            nn.GroupNorm(8, in_channels)\n","        )\n","        self.resnet_blocks = nn.ModuleList([ResNetBlock(in_channels) for _ in range(num_resnet_block)])\n","        self.attention = SelfAttention(in_channels) if allow_attention else None\n","        self.conv = Downsample(in_channels, out_channels) if allow_conv else None\n","        # self.in_channels = in_channels\n","\n","    def forward(self, x, time_emb=None):\n","        if self.mlp is not None:\n","        # print('\\t', 'mlp_start', psutil.virtual_memory()[1]/1024000000)\n","            time_emb = self.mlp(time_emb)\n","            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n","            scale_shift = time_emb.chunk(2, dim=1)\n","            x = self.scaling(x) * (scale_shift[0] + 1) + scale_shift[1]\n","            x = nn.SiLU()(x)\n","        # time_emb.detach()\n","        # print('\\t', 'mlp_done', psutil.virtual_memory()[1]/1024000000)\n","        for block in self.resnet_blocks:\n","            # print('\\t', 'block_start', psutil.virtual_memory()[1]/1024000000)\n","            x = block(x)\n","            # print('\\t', 'block_done', psutil.virtual_memory()[1]/1024000000)\n","        if self.attention is not None:\n","            # print('\\t', 'attention_start', psutil.virtual_memory()[1]/1024000000)\n","            x = self.attention(x)\n","            # print('\\t', 'attention_done', psutil.virtual_memory()[1]/1024000000)\n","        if self.conv is not None:\n","            # print('\\t', 'Conv_start', psutil.virtual_memory()[1]/1024000000)\n","            # print(x.shape)\n","            x = self.conv(x)\n","            # print(x.shape)\n","            # print('\\t', 'Conv_done', psutil.virtual_memory()[1]/1024000000)\n","        return x\n","\n","\n","class EfficientUNet(nn.Module):\n","    '''\n","    Efficient UNet architecture\n","    '''\n","    def __init__(\n","            self,\n","            in_channels=4,\n","            out_channels=1,\n","            channels=[64,128,256,512,1024],\n","            num_resnet_blocks=[1,2,4,8,8],\n","            skips=[True,True,True,True,False],\n","            # channels=[128,256],\n","            # num_resnet_blocks=[1,1],\n","            # skips=[True,False]\n","        ):\n","        super(EfficientUNet, self).__init__()\n","        self.skips = skips\n","        self.downs = nn.ModuleList()\n","        self.ups = nn.ModuleList()\n","\n","        out_chan = 32\n","        self.time_mlp = nn.Sequential(\n","            SinusoidalPositionEmbeddings(out_chan),\n","            nn.Linear(out_chan, 256),\n","            nn.GELU(),\n","            nn.Linear(256, 256),\n","        )\n","\n","        # Input - Initial convolution\n","        self.input = nn.Conv2d(in_channels, out_chan, kernel_size=3, padding=1)\n","\n","        # Down part\n","        in_chan = out_chan\n","        for num_resnet_block, channel, skip in zip(num_resnet_blocks[:-1], channels[:-1], skips[:-1]):\n","            out_chan = channel\n","            self.downs.append(EncoderBlock(in_chan, out_chan, num_resnet_block=num_resnet_block, allow_conv=True))\n","            # self.downs.append(EncoderBlock(in_chan, out_chan, num_resnet_block=num_resnet_block, allow_conv=True, allow_attention=not skip))\n","            in_chan = out_chan\n","\n","        # Middle part\n","        out_chan = channels[-1]\n","        self.middle = EncoderBlock(in_chan, out_chan, num_resnet_block=8, allow_conv=True, allow_attention=True)\n","        in_chan = out_chan\n","\n","        # Up part\n","        for num_resnet_block, channel, skip in zip(reversed(num_resnet_blocks[:-1]), reversed(channels[:-1]), reversed(skips[:-1])):\n","            out_chan = channel\n","            self.ups.append(DecoderBlock(in_chan, out_chan, num_resnet_block=num_resnet_block, allow_conv=True))\n","            in_chan = out_chan\n","\n","        # Output - Compress to 1 channel\n","        self.output = nn.Linear(in_chan, out_channels)\n","\n","    def forward(self, x, time):\n","        # print('UNet Start', psutil.virtual_memory()[1]/1024000000)\n","        skip_connections = []\n","        t = self.time_mlp(time)\n","        # print('Time MLP', psutil.virtual_memory()[1]/1024000000)\n","        x = self.input(x)\n","        for skip, down in zip(self.skips, self.downs):\n","            # print('Down', psutil.virtual_memory()[1]/1024000000)\n","            x = down(x, t)\n","            if skip:\n","                skip_connections.append(x)\n","            else:\n","                skip_connections.append(None)\n","        # print(skip_connections[0].shape)\n","        for skip, up in zip(reversed(skip_connections), self.ups):\n","            # print('Up', psutil.virtual_memory()[1]/1024000000)\n","            if skip is not None:\n","                x = torch.cat((x, skip), dim=1)\n","            x = up(x, t)\n","        # print(x.shape)\n","        x = self.output(x.transpose(1, 2).transpose(2, 3)).transpose(2,3).transpose(1,2)\n","        # print(x.shape)\n","        # print('UNet End', psutil.virtual_memory()[1]/1024000000)\n","\n","        return x"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":83653,"status":"ok","timestamp":1690969361210,"user":{"displayName":"Melvin Tong (Melvintong)","userId":"09425469311594841840"},"user_tz":-480},"id":"ylVZKWp_SirA"},"outputs":[],"source":["def extract(a, t, x_shape):\n","    batch_size = t.shape[0]\n","    t = t.to(a.device)\n","    out = a.gather(-1, t)\n","    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n","\n","# def noise_like(shape, device, repeat=False):\n","#     def repeat_noise(): return torch.randn(\n","#         (1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n","\n","#     def noise(): return torch.randn(shape, device=device)\n","\n","#     return repeat_noise() if repeat else noise()\n","\n","def warm_up_beta(linear_start, linear_end, timesteps, warm_up_frac):\n","    betas = linear_end * torch.ones(timesteps, dtype=np.float32)\n","    warm_up_steps = int(timesteps * warm_up_frac)\n","    betas[:warm_up_steps] = torch.linspace(linear_start, linear_end, warm_up_steps, dtype=np.float32)\n","    return betas\n","\n","def make_beta_schedule(schedule, timesteps, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n","    if schedule == 'warmup10':\n","        betas = warm_up_beta(linear_start, linear_end, timesteps, 10)\n","    elif schedule == 'warmup20':\n","        betas = warm_up_beta(linear_start, linear_end, timesteps, 20)\n","    elif schedule == 'warmup50':\n","        betas = warm_up_beta(linear_start, linear_end, timesteps, 50)\n","    elif schedule == 'linear':\n","        betas = torch.linspace(linear_start, linear_end, timesteps, dtype=np.float32)\n","    elif schedule == 'quad':\n","        betas = torch.linspace(linear_start ** 0.5, linear_end ** 0.5, timesteps, dtype=np.float32) ** 2\n","    elif schedule == 'const':\n","        betas = linear_end * torch.ones(timesteps, dtype=np.float32)\n","    elif schedule == 'cosine':\n","        steps = timesteps + 1\n","        x = torch.linspace(0, timesteps, steps)\n","        alphas = torch.cos(((x / timesteps) + cosine_s) / (1 + cosine_s) * torch.pi * 0.5) ** 2\n","        alphas = alphas / alphas[0]\n","        betas = 1 - (alphas[1:] / alphas[:-1])\n","        betas = torch.clip(betas, linear_start, 0.999)\n","    else:\n","        raise ValueError('Unknown beta schedule')\n","\n","    return betas\n","\n","\n","class StepUnrolledDiffusion(nn.Module):\n","    def __init__(self, schedule_name, model, optimizer, loss_fn, device, timesteps=1000):\n","        super(StepUnrolledDiffusion, self).__init__()\n","        self.schedule = make_beta_schedule(schedule_name, timesteps)\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.loss_fn = loss_fn\n","        self.device = device\n","        self.timesteps = timesteps\n","\n","        betas = self.schedule.detach().cpu().numpy() if isinstance(self.schedule, torch.Tensor) else self.schedule\n","        alphas = 1. - betas\n","        alphas_cumprod = np.cumprod(alphas, 0)\n","        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n","        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n","\n","        self.betas = torch.from_numpy(betas).to(device)\n","        self.alphas_cumprod = torch.from_numpy(alphas_cumprod).to(device)\n","        # self.alphas_cumprod_prev = torch.from_numpy(alphas_cumprod_prev).to(device)\n","        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n","        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n","        # self.log_one_minus_alphas_cumprod = torch.log(1. - self.alphas_cumprod)\n","        self.sqrt_recip_alphas_cumprod = torch.from_numpy(np.sqrt(1. / alphas_cumprod)).to(device)\n","        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1. / self.alphas_cumprod - 1)\n","        self.posterior_variance = torch.from_numpy(posterior_variance).to(device)\n","        self.posterior_log_variance_clipped = torch.log(torch.clamp(self.posterior_variance, min=1e-20))\n","        self.posterior_mean_coef1 = torch.from_numpy(betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)).to(device)\n","        self.posterior_mean_coef2 = torch.from_numpy((1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)).to(device)\n","\n","    @torch.no_grad()\n","    def denoise(self, x, t):\n","        mean = extract(self.sqrt_alphas_cumprod, t, x.shape) * x\n","        variance = 1. - self.alphas_cumprod\n","        log_variance = self.posterior_log_variance_clipped\n","        noise = torch.randn(x.shape, device=self.device, dtype=torch.float32)\n","        x = (mean + self.sqrt_recipm1_alphas_cumprod * noise * torch.exp(0.5 * log_variance)).to(torch.float32)\n","        for i in reversed(range(self.timesteps)):\n","            output = self.model(x, torch.full((x.shape[0],), i, device=self.device, dtype=torch.long))\n","            x = (x - self.posterior_mean_coef1[i] * output - self.posterior_mean_coef2[i] * output) / self.sqrt_one_minus_alphas_cumprod[i]\n","        return x\n","\n","    # def denoise_depth(self, x):\n","    #     rgb_channels, depth_channel = x[:,:3,:,:], x[:,3,:,:]\n","    #     mean = self.sqrt_recip_alphas_cumprod * depth_channel\n","    #     variance = 1. - self.alphas_cumprod\n","    #     log_variance = self.posterior_log_variance_clipped\n","    #     noise = noise_like(depth_channel.shape, self.device)\n","    #     depth_channel = mean + self.sqrt_recipm1_alphas_cumprod * noise * torch.exp(0.5 * log_variance)\n","    #     for i in reversed(range(self.timesteps)):\n","    #         depth_channel = (depth_channel - self.posterior_mean_coef1[i] * self.model(depth_channel, i) - self.posterior_mean_coef2[i] * self.model(depth_channel, i)) / self.sqrt_one_minus_alphas_cumprod[i]\n","    #     return torch.cat((rgb_channels, depth_channel), dim=1)\n","\n","\n","    # def q_mean_variance(self, t, x_start):\n","    #     # rgb_channels, depth_channel = x_start[:,:3,:,:], x_start[:,3,:,:]\n","    #     # mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * depth_channel\n","    #     # variance = extract(1. - self.alphas_cumprod, t, x_start.shape)\n","    #     # log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n","    #     # return torch.cat((rgb_channels, mean), dim=1), torch.cat((rgb_channels, variance), dim=1), torch.cat((rgb_channels, log_variance), dim=1)\n","    #     mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n","    #     variance = extract(1. - self.alphas_cumprod, t, x_start.shape)\n","    #     log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n","    #     return mean, variance, log_variance\n","\n","    # def q_sample(self, x_t, t, noise=None):\n","    #     if noise is None:\n","    #         noise = torch.randn_like(x_t).to(self.device).to(torch.float32)\n","    #     return extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t + extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * noise\n","\n","    @torch.no_grad()\n","    def q_posterior(self, x_start, x_t, t):\n","        coef1 = extract(self.posterior_mean_coef1, t, x_t.shape)\n","        posterior_mean = (extract(self.posterior_mean_coef1, t, x_t.shape) * x_start + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t)\n","        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n","        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n","        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n","\n","    @torch.no_grad()\n","    def p_mean_variance(self, x, t, clip_denoised=False):\n","        x_recon = extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - extract(self.sqrt_recipm1_alphas_cumprod, t, x.shape) * self.denoise(x, t)\n","        if clip_denoised:\n","            x_recon = torch.clamp(x_recon, -1., 1.)\n","        model_mean, model_variance, model_log_variance = self.q_posterior(x, x_recon, t)\n","        return model_mean, model_variance, model_log_variance\n","\n","    @torch.no_grad()\n","    def p_sample(self, x, t, clip_denoised=False, repeat_noise=False):\n","        # rgb_channels, depth_channel = x[:,:3,:,:], x[:,3,:,:]\n","        # batch, *_, device = *depth_channel.shape, depth_channel.device\n","        # mean, _, log_variance = self.p_mean_variance(depth_channel, t, clip_denoised)\n","        # noise = noise_like(depth_channel.shape, device, repeat=repeat_noise)\n","        # nonzero_mask = (1. - (t == 0).float()).reshape(batch, *((1,) * len(depth_channel.shape - 1)))\n","        # if t == torch.tensor([0]):\n","        #     return torch.cat((rgb_channels, mean), dim=1)\n","        # return torch.cat((rgb_channels, mean + torch.exp(0.5 * log_variance) * noise * nonzero_mask), dim=1)\n","        # batch, device = x.shape[0], x.device\n","        model_mean, _, model_log_variance = self.p_mean_variance(x, t, clip_denoised=clip_denoised)\n","        noise = torch.randn((1, *x.shape[1:]), device=x.device).repeat(x.shape[0], *((1,) * (len(x.shape) - 1)))\n","        nonzero_mask = (1. - (t == 0).float()).reshape(x.shape[0], *((1,) * (len(x.shape) - 1)))\n","        # if t == torch.tensor([0]):\n","        #     return model_mean\n","        return model_mean + torch.exp(0.5 * model_log_variance) * noise * nonzero_mask\n","\n","    @torch.no_grad()\n","    def p_sample_loop(self, x, denoising_steps, sample_interval):\n","        device = self.betas.device\n","        img = torch.randn(x.shape, device=device)\n","        ret_img = img[:,3,:,:].unsqueeze(1)\n","        # for i in tqdm(reversed(range(denoising_steps)), desc='sampling loop timestep', total=denoising_steps):\n","        for i in reversed(range(denoising_steps)):\n","            img = self.p_sample(img, torch.full((x.shape[0],), i, device=device, dtype=torch.long), clip_denoised=True)[:,3,:,:].unsqueeze(1)\n","            if (i+1) % sample_interval == 0:\n","                ret_img = torch.cat([ret_img, img], dim=0)\n","        return ret_img\n","\n","    @torch.no_grad()\n","    def sample(self, rgb_photo, denoising_steps, sample_interval):\n","        return self.p_sample_loop(torch.cat((rgb_photo.to(self.betas.device), torch.randn((rgb_photo.shape[0], 1, rgb_photo.shape[2], rgb_photo.shape[3]), device=self.betas.device)), dim=1), denoising_steps, sample_interval)\n","\n","    def forward(self, x, timesteps=1000, finetune=False):\n","        rgb_channels, depth_channel = x[:,:3,:,:], x[:,3,:,:].unsqueeze(1)\n","        if not finetune:\n","            for i in range(timesteps):\n","                # print('forward', i, psutil.virtual_memory()[1]/1024000000)\n","                noise = torch.randn(depth_channel.shape, device=self.device)\n","                depth_channel = depth_channel + torch.sqrt(self.betas[i]).to(self.device) * noise\n","                # print('Pre-pred', psutil.virtual_memory()[1]/1024000000)\n","                pred_depth = self.model(torch.cat((rgb_channels, depth_channel), dim=1), torch.full((x.shape[0],), i, device=self.device, dtype=torch.long))\n","            return pred_depth\n","        else:\n","            pred_depth = self.model(x, torch.full((x.shape[0],), 0, device=self.device, dtype=torch.long))\n","            for i in range(1, timesteps):\n","                noise = torch.randn(depth_channel.shape, device=self.device)\n","                pred_depth = pred_depth + torch.sqrt(self.betas[i]) * torch.randn(depth_channel.shape, device=self.device)\n","                pred_depth = self.model(torch.cat((rgb_channels, pred_depth), dim=1), torch.full((x.shape[0],), i, device=self.device, dtype=torch.long))\n","            return pred_depth\n","\n","    # def step(self, x, y, finetune=False):\n","    #     loss = self.loss_fn(self.forward(x, finetune=finetune), y)\n","    #     loss.backward()\n","    #     self.optimizer.step()\n","    #     return loss\n","\n","    def train(self, train_loader, epochs=10, timesteps=1000, finetune=False):\n","        self.model.train()\n","        # print('Train begin', psutil.virtual_memory()[1]/1024000000)\n","        for epoch in range(epochs):\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            for i, batch in tqdm(enumerate(train_loader), desc=f'Training epoch {epoch+1}', total=len(train_loader), unit='batch'):\n","            # for i, batch in enumerate(train_loader):\n","                self.optimizer.zero_grad()\n","                image = batch[0].to(self.device)\n","                depth = batch[1].to(self.device)\n","                image_with_depth = Variable(torch.cat((image, depth), dim=1), requires_grad=True)\n","                depth = Variable(depth, requires_grad=True)\n","                output = self.forward(image_with_depth, timesteps=timesteps, finetune=finetune)\n","                loss = self.loss_fn(output, depth)\n","                loss.backward()\n","                self.optimizer.step()\n","                print(f'Epoch: {epoch+1}, Batch: {i}, Loss: {loss.item()}')\n","            if not (epoch + 1) % 10:\n","                torch.save(self.model.state_dict(), f'./models/epoch_{epoch+1}.pth')\n","\n","    @torch.no_grad()\n","    def test(self, test_loader, denoising_steps=128, sample_interval=32):\n","        self.model.eval()\n","        pred_list = []\n","        depth_list = []\n","        # for i, batch in tqdm(enumerate(test_loader), desc='Testing', total=len(test_loader), unit='batch'):\n","        for i, batch in enumerate(test_loader):\n","            image = batch[0].to(self.device)\n","            depth = batch[1].to(self.device)\n","            pred = self.sample(image, denoising_steps=denoising_steps, sample_interval=sample_interval)[::2,:,:,:]\n","            np.save(f'./results/pred_{i}.npy', pred.cpu().numpy())\n","            pred_list.append(pred)\n","            np.save(f'./results/depth_{i}.npy', depth.cpu().numpy())\n","            depth_list.append(depth)\n","            # for j in range(batch_size):\n","            #     np.save(f'./results/pred_{i*batch_size+j}.npy', pred[j].cpu().numpy())\n","            #     np.save(f'./results/depth_{i*batch_size+j}.npy', depth[j].cpu().numpy())\n","                # pred_list.append(pred[j].cpu().numpy())\n","                # depth_list.append(depth[j].cpu().numpy())\n","            \n","            # save the predicted depth map\n","            # pred = pred.cpu().numpy()\n","            # depth = depth.cpu().numpy()\n","            # for j in range(pred.shape[0]):\n","            #     np.save(f'./results/pred_{i*8+j}.npy', pred[j])\n","            #     np.save(f'./results/depth_{i*8+j}.npy', depth[j])\n","\n","\n","        pred = np.stack([pred_item.squeeze(1).cpu().numpy() for pred_item in pred_list])\n","        depth = np.stack([depth_item.squeeze(1).cpu().numpy() for depth_item in depth_list])\n","\n","        # accuracy under the threshold 1.25^i, i = 1, 2, 3, min=0, max=1\n","        print('Accuracy under threshold δ_i < 1.25^i')\n","        for i in range(1, 4):\n","            threshold = 1.25 ** i\n","            accuracy = np.sum(np.maximum(pred / depth, depth / pred) < threshold) / pred.shape[0] / pred.shape[1] / pred.shape[2]\n","            print(f'δ_{i}: {accuracy} (higher is better)')\n","        \n","        # absolute relative error\n","        rel = np.sum(np.abs(pred - depth) / depth) / pred.shape[0] / pred.shape[1] / pred.shape[2]\n","        print(f'Absolute relative error: {rel} (lower is better)')\n","\n","        # root mean squared error\n","        rms = np.sqrt(np.sum((pred - depth) ** 2) / pred.shape[0] / pred.shape[1] / pred.shape[2])\n","        print(f'Root mean squared error: {rms} (lower is better)')\n","\n","        # # absolute error of log depth\n","        # abs_log = np.sum(np.log(np.abs(pred / depth))) / pred.shape[0] / pred.shape[1] / pred.shape[2]\n","        # print(f'Absolute error of log depth: {abs_log} (lower is better)')\n","            \n","\n","class NYUDv2Dataset(Dataset):\n","    def __init__(self, device='cpu', sample=[0,2899]):\n","        super(NYUDv2Dataset, self).__init__()\n","        images = {'image': [], 'depth': []}\n","        image_dir = './dataset/NYUDv2/image_cleaned_resized/'\n","        depth_dir = './dataset/NYUDv2/depth_cleaned_resized/'\n","        # image_dir = './drive/MyDrive/Colab Notebooks/MDEuDM/dataset/NYUDv2/image_cleaned_resized/'\n","        # depth_dir = './drive/MyDrive/Colab Notebooks/MDEuDM/dataset/NYUDv2/depth_cleaned_resized/'\n","        for file in os.listdir(image_dir)[sample[0]:sample[1]]:\n","            if file.endswith('.npy'):\n","                image = np.load(image_dir + file).transpose(2, 0, 1)\n","                image = torch.from_numpy(image)\n","                images['image'].append(image)\n","        for file in os.listdir(depth_dir)[sample[0]:sample[1]]:\n","            if file.endswith('.npy'):\n","                depth = np.load(depth_dir + file)\n","                depth = torch.from_numpy(depth).unsqueeze(0)\n","                images['depth'].append(depth)\n","        self.x = torch.stack(images['image']).to(device)\n","        self.y = torch.stack(images['depth']).to(device)\n","        self.n_samples = len(self.x)\n","\n","    def __getitem__(self, index):\n","        return [self.x[index], self.y[index]]\n","\n","    def __len__(self):\n","        return self.n_samples"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["training start\n","Epoch: 0\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["gc.enable()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# training = NYUDv2Dataset(sample=[0, 2], device=device)\n","# training_loader = DataLoader(training, batch_size=2, shuffle=False)\n","# finetuning = NYUDv2Dataset(sample=[2, 4], device=device)\n","# finetuning_loader = DataLoader(finetuning, batch_size=2, shuffle=False)\n","# testing = NYUDv2Dataset(sample=[4, 6], device=device)\n","# testing_loader = DataLoader(testing, batch_size=2, shuffle=False)\n","training = NYUDv2Dataset(sample=[0, 2720], device=device)\n","training_loader = DataLoader(training, batch_size=8, shuffle=False)\n","# finetuning = NYUDv2Dataset(sample=[2720, 2800], device=device)\n","# finetuning_loader = DataLoader(finetuning, batch_size=8, shuffle=False)\n","# testing = NYUDv2Dataset(sample=[2850, 2899], device=device)\n","# testing_loader = DataLoader(testing, batch_size=8, shuffle=False)\n","timesteps = 1000\n","unet_model = EfficientUNet()\n","optimizer = Adam(unet_model.parameters(), lr=1e-4)\n","loss_fn = nn.SmoothL1Loss()\n","model = StepUnrolledDiffusion('cosine', unet_model, optimizer, loss_fn, device, timesteps=timesteps)\n","if torch.cuda.is_available():\n","    unet_model = nn.DataParallel(unet_model)\n","    model = nn.DataParallel(model)\n","print(\"training start\")\n","# # print(psutil.virtual_memory()[1]/1024000000)\n","model.train(training_loader, epochs=100, timesteps=timesteps, finetune=False)\n","# print(\"finetuning start\")\n","# model.train(finetuning_loader, epochs=1, timesteps=timesteps, finetune=True)\n","# print(\"testing start\")\n","# model.test(testing_loader, denoising_steps=1, sample_interval=1)\n","print(\"done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":1709,"status":"error","timestamp":1690970459782,"user":{"displayName":"Melvin Tong (Melvintong)","userId":"09425469311594841840"},"user_tz":-480},"id":"NTb1zAfkYh8r","outputId":"b717e024-e9f9-41f7-cd94-17ff6842f9a5"},"outputs":[{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-093eece0a49d>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# plot([get_noisy_image(image, torch.tensor([t])) for t in [0, 99, 199, 299, 399, 499, 599, 699, 799, 899, 999]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_noisy_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m199\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m399\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m599\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m799\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m# plot([get_noisy_image(image, torch.tensor([t])) for t in [0, 49, 99, 149, 199]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-093eece0a49d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# plot([get_noisy_image(image, torch.tensor([t])) for t in [0, 99, 199, 299, 399, 499, 599, 699, 799, 899, 999]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_noisy_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m199\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m399\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m599\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m799\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m# plot([get_noisy_image(image, torch.tensor([t])) for t in [0, 49, 99, 149, 199]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-093eece0a49d>\u001b[0m in \u001b[0;36mget_noisy_image\u001b[0;34m(x, t)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_noisy_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-093eece0a49d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      8\u001b[0m reverse_transform = Compose([\n\u001b[1;32m      9\u001b[0m     \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"]}],"source":["# from PIL import Image\n","\n","# transform = Compose([\n","#     ToTensor(),\n","#     Lambda(lambda t: (t * 2) - 1),\n","# ])\n","\n","# reverse_transform = Compose([\n","#     Lambda(lambda t: (t + 1) / 2),\n","#     Lambda(lambda t: t.permute(1,2,0)),\n","#     Lambda(lambda t: t * 255.),\n","#     Lambda(lambda t: t.numpy().astype(np.uint8)),\n","#     # ToPILImage(),\n","# ])\n","\n","# image = np.load('./drive/MyDrive/Colab Notebooks/MDEuDM/dataset/NYUDv2/depth_cleaned_resized/0.npy')\n","# image = Image.fromarray(image.astype('uint8')).convert('L')\n","# # display(image)\n","# image = transform(image).unsqueeze(0).to(device)\n","\n","# def get_noisy_image(x, t):\n","#   x = model.q_sample(x, t.to(device))\n","#   x = reverse_transform(x.squeeze().cpu())\n","#   return x\n","\n","# def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n","#     if not isinstance(imgs[0], list):\n","#         # Make a 2d grid even if there's just 1 row\n","#         imgs = [imgs]\n","\n","#     num_rows = len(imgs)\n","#     num_cols = len(imgs[0]) + with_orig\n","#     fig, axs = plt.subplots(figsize=(320,240), nrows=num_rows, ncols=num_cols, squeeze=False)\n","#     for row_idx, row in enumerate(imgs):\n","#         row = [image] + row if with_orig else row\n","#         for col_idx, img in enumerate(row):\n","#             ax = axs[row_idx, col_idx]\n","#             ax.imshow(np.asarray(img), **imshow_kwargs)\n","#             ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","\n","#     if with_orig:\n","#         axs[0, 0].set(title='Original image')\n","#         axs[0, 0].title.set_size(8)\n","#     if row_title is not None:\n","#         for row_idx in range(num_rows):\n","#             axs[row_idx, 0].set(ylabel=row_title[row_idx])\n","\n","#     plt.tight_layout()\n","\n","# # plot([get_noisy_image(image, torch.tensor([t])) for t in [0, 99, 199, 299, 399, 499, 599, 699, 799, 899, 999]])\n","# plot([get_noisy_image(image, torch.tensor([t])) for t in [0, 199, 399, 599, 799, 999]])\n","# # plot([get_noisy_image(image, torch.tensor([t])) for t in [0, 49, 99, 149, 199]])"]}],"metadata":{"colab":{"collapsed_sections":["AeYBRJQ023H5"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
